{
 "cells": [
  {
   "cell_type": "code",
   "id": "df35c232",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2025-08-15T05:06:16.729105Z",
     "start_time": "2025-08-15T05:06:10.501031Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from utils.clean import get_azerbaijani_dataset\n",
    "from utils.config import ROOT_DIR"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hikme\\Desktop\\Programming Projects\\Personal Projects\\AI Proejcts\\azerbaijani-tokenizer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hikme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T05:06:16.737342Z",
     "start_time": "2025-08-15T05:06:16.734248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TOKENIZER_PATH = f\"{ROOT_DIR}/tokenizer\"\n",
    "\n",
    "os.makedirs(TOKENIZER_PATH, exist_ok=True)"
   ],
   "id": "71a09e5498f5e7d7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T12:33:51.870766Z",
     "start_time": "2025-08-14T12:33:51.867961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_NAMES = (\"LocalDoc/AzTC\", \"allmalab/DOLLMA\")\n",
    "\n",
    "get_azerbaijani_dataset(DATASET_NAMES)"
   ],
   "id": "451657769b213c0f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-08-15T05:11:07.568012600Z",
     "start_time": "2025-08-15T05:06:17.024034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = dict(\n",
    "  # Input spec\n",
    "  input=f\"{ROOT_DIR}/data/azerbaijani_data.txt\",\n",
    "  input_format=\"text\",\n",
    "  # Output spec\n",
    "  model_prefix=f\"{TOKENIZER_PATH}/azerbaijani_unigram\",\n",
    "  # Algorithm spec\n",
    "  model_type=\"unigram\",\n",
    "  vocab_size=40_000,\n",
    "  # Normalization\n",
    "  normalization_rule_name=\"identity\", # No normalization applied\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=10_000_000, # Set to 0 zero to train on the whole dataset\n",
    "  max_sentence_length=4192, # Max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1_000_000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # Rare word handling\n",
    "  character_coverage=0.9995,\n",
    "  byte_fallback=True,\n",
    "  # Merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  train_extremely_large_corpus=True,\n",
    "  # Special tokens\n",
    "  unk_id=0, # the UNK token must exist\n",
    "  bos_id=1, # The others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # Systems\n",
    "  num_threads=os.cpu_count()\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ],
   "id": "d6aaed7a99d33d6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T04:52:49.041426200Z",
     "start_time": "2025-08-14T12:21:55.668633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f\"{TOKENIZER_PATH}/azerbaijani_unigram.model\")\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]"
   ],
   "id": "456c384240e3c8f7",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T04:52:49.048779400Z",
     "start_time": "2025-08-14T12:24:08.509512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "azerbaijani_samples = [\n",
    "        \"Kitab deyəndə burda mədəniyyət, incəsənət nümunələri nəzərdə tuturam.\",\n",
    "        \"Uzun müddətdir ki, cəbhədə rəsmi olaraq atəşkəs hökm sürür.\",\n",
    "        \"Biz Bütöv Azərbaycançıyıq, bunu öz əməllərimizdə göstərmişik.\"\n",
    "]"
   ],
   "id": "d3968be7debd8be6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T04:52:49.049778900Z",
     "start_time": "2025-08-14T12:27:49.466877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for sentence in azerbaijani_samples:\n",
    "    ids = sp.encode(sentence)\n",
    "    print([sp.id_to_piece(idx) for idx in ids])"
   ],
   "id": "efee8638164a40e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Kitab', '▁deyəndə', '▁burda', '▁mədəniyyət', ',', '▁incəsənət', '▁nümunələri', '▁nəzərdə', '▁tuturam', '.']\n",
      "['▁Uzun', '▁müddətdir', '▁ki', ',', '▁cəbhədə', '▁rəsmi', '▁olaraq', '▁atəşkəs', '▁hökm', '▁sürür', '.']\n",
      "['▁Biz', '▁Bütöv', '▁Azərbaycan', 'çı', 'yıq', ',', '▁bunu', '▁öz', '▁əməl', 'lərimizdə', '▁göstər', 'mişik', '.']\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
